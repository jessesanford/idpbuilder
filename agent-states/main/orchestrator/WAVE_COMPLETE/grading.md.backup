# Orchestrator - WAVE_COMPLETE State Grading

## Critical Performance Metrics

┌─────────────────────────────────────────────────────────────────┐
│ PRIMARY METRIC: Wave Completion Accuracy                      │
├─────────────────────────────────────────────────────────────────┤
│ Measurement: Percentage of quality gates passed              │
│ Target: 100% of quality gates must pass                      │
│ Grade: PASS/FAIL (binary)                                    │
│ Weight: 70% of overall orchestrator grade                    │
│ Consequence: FAIL = Wave invalid, rework required            │
└─────────────────────────────────────────────────────────────────┘

## Grading Rubric

| Metric | Excellent | Good | Acceptable | FAIL |
|--------|-----------|------|------------|------|
| Quality Gates Passed | 100% | 100% | 100% | <100% |
| Validation Speed | <5 min | 5-10 min | 10-15 min | >15 min |
| Integration Success | Clean merge | Minor conflicts | Resolved conflicts | Unresolved conflicts |
| Decision Accuracy | 100% correct | 95% correct | 90% correct | <90% |
| Report Completeness | 100% complete | 95% complete | 90% complete | <90% |

## Real-Time Scoring

```python
class WaveCompleteGrader:
    def __init__(self):
        self.wave_completions = []
        self.quality_gate_history = []
        
    def grade_wave_completion(self, completion_data):
        """Grade a wave completion cycle"""
        
        # Critical: Quality gate validation
        quality_grade = self.calculate_quality_gate_grade(completion_data)
        
        # Validation speed and efficiency
        speed_grade = self.calculate_validation_speed(completion_data)
        
        # Integration success
        integration_grade = self.evaluate_integration_success(completion_data)
        
        # Decision accuracy (next state selection)
        decision_grade = self.evaluate_decision_accuracy(completion_data)
        
        # Report completeness and accuracy
        report_grade = self.evaluate_report_quality(completion_data)
        
        overall = self.calculate_overall_grade(
            quality_grade, speed_grade, integration_grade,
            decision_grade, report_grade
        )
        
        return {
            'quality_gates': quality_grade,
            'validation_speed': speed_grade,
            'integration': integration_grade,
            'decision': decision_grade,
            'reporting': report_grade,
            'overall': overall,
            'timestamp': datetime.now().isoformat()
        }
    
    def calculate_quality_gate_grade(self, completion):
        """Calculate quality gate validation performance"""
        
        quality_results = completion.get('quality_gates', {})
        gates_passed = quality_results.get('pass_count', 0)
        total_gates = quality_results.get('total_gates', 1)
        
        pass_percentage = (gates_passed / total_gates) * 100
        
        # Quality gates are binary - must be 100%
        if pass_percentage == 100:
            grade = 'EXCELLENT'
            score = 100
        else:
            grade = 'FAIL'
            score = 0
        
        failed_gates = quality_results.get('failed_gates', [])
        
        return {
            'pass_percentage': pass_percentage,
            'gates_passed': gates_passed,
            'total_gates': total_gates,
            'failed_gates': failed_gates,
            'grade': grade,
            'score': score
        }
    
    def calculate_validation_speed(self, completion):
        """Calculate wave validation completion speed"""
        
        validation_start = completion.get('validation_started_at')
        validation_end = completion.get('validation_completed_at')
        
        if not validation_start or not validation_end:
            return {
                'duration_minutes': 0,
                'grade': 'EXCELLENT',
                'score': 100
            }
        
        start_time = datetime.fromisoformat(validation_start.replace('Z', '+00:00'))
        end_time = datetime.fromisoformat(validation_end.replace('Z', '+00:00'))
        
        duration_minutes = (end_time - start_time).total_seconds() / 60
        
        if duration_minutes < 5:
            grade = 'EXCELLENT'
            score = 100
        elif duration_minutes < 10:
            grade = 'GOOD'
            score = 90
        elif duration_minutes < 15:
            grade = 'PASS'
            score = 75
        else:
            grade = 'FAIL'
            score = 0
        
        return {
            'duration_minutes': duration_minutes,
            'grade': grade,
            'score': score
        }
    
    def evaluate_integration_success(self, completion):
        """Evaluate wave integration success"""
        
        integration = completion.get('integration_results', {})
        
        conflicts = integration.get('conflicts_detected', 0)
        resolution_time = integration.get('resolution_time_minutes', 0)
        final_status = integration.get('final_status', 'SUCCESS')
        
        if final_status != 'SUCCESS':
            grade = 'FAIL'
            score = 0
        elif conflicts == 0:
            grade = 'EXCELLENT'
            score = 100
        elif conflicts <= 2 and resolution_time <= 30:
            grade = 'GOOD'
            score = 85
        elif conflicts <= 5 and resolution_time <= 60:
            grade = 'PASS'
            score = 70
        else:
            grade = 'FAIL'
            score = 0
        
        return {
            'conflicts': conflicts,
            'resolution_time_minutes': resolution_time,
            'final_status': final_status,
            'grade': grade,
            'score': score
        }
    
    def evaluate_decision_accuracy(self, completion):
        """Evaluate accuracy of next state decision"""
        
        decision_data = completion.get('next_state_decision', {})
        
        predicted_next_state = decision_data.get('next_state')
        actual_next_state = decision_data.get('actual_next_state')
        decision_rationale = decision_data.get('reason', '')
        
        if not predicted_next_state or not actual_next_state:
            return {
                'decision_accuracy': 'UNKNOWN',
                'grade': 'PASS',
                'score': 75  # Default score when no comparison possible
            }
        
        # Check if decision was correct
        if predicted_next_state == actual_next_state:
            accuracy = 100
            grade = 'EXCELLENT'
        elif self.are_states_related(predicted_next_state, actual_next_state):
            accuracy = 80  # Reasonable alternate choice
            grade = 'GOOD'
        else:
            accuracy = 0
            grade = 'FAIL'
        
        # Factor in rationale quality
        rationale_quality = self.evaluate_decision_rationale(decision_rationale)
        
        # Adjust score based on rationale
        adjusted_score = (accuracy * 0.8) + (rationale_quality * 0.2)
        
        if adjusted_score >= 95:
            final_grade = 'EXCELLENT'
        elif adjusted_score >= 85:
            final_grade = 'GOOD'
        elif adjusted_score >= 70:
            final_grade = 'PASS'
        else:
            final_grade = 'FAIL'
        
        return {
            'predicted_state': predicted_next_state,
            'actual_state': actual_next_state,
            'accuracy': accuracy,
            'rationale_quality': rationale_quality,
            'adjusted_score': adjusted_score,
            'grade': final_grade,
            'score': adjusted_score
        }
    
    def evaluate_report_quality(self, completion):
        """Evaluate wave completion report quality"""
        
        report = completion.get('completion_report', {})
        
        required_sections = [
            'wave_id', 'completion_timestamp', 'summary',
            'effort_details', 'performance_metrics', 'quality_gates'
        ]
        
        completeness_score = 0
        for section in required_sections:
            if section in report and report[section]:
                completeness_score += (100 / len(required_sections))
        
        # Check report accuracy
        accuracy_score = self.validate_report_accuracy(report, completion)
        
        # Check report usefulness
        usefulness_score = self.evaluate_report_usefulness(report)
        
        # Weighted score
        overall_score = (
            completeness_score * 0.4 +
            accuracy_score * 0.4 +
            usefulness_score * 0.2
        )
        
        if overall_score >= 95:
            grade = 'EXCELLENT'
        elif overall_score >= 85:
            grade = 'GOOD'
        elif overall_score >= 75:
            grade = 'PASS'
        else:
            grade = 'FAIL'
        
        return {
            'completeness': completeness_score,
            'accuracy': accuracy_score,
            'usefulness': usefulness_score,
            'overall_score': overall_score,
            'grade': grade,
            'score': overall_score
        }
    
    def calculate_overall_grade(self, quality, speed, integration, decision, reporting):
        """Calculate weighted overall grade"""
        
        # Quality gates: 70% (critical - must pass)
        # Integration: 15%
        # Speed: 8%
        # Decision: 4%
        # Reporting: 3%
        
        weighted_score = (
            quality['score'] * 0.70 +
            integration['score'] * 0.15 +
            speed['score'] * 0.08 +
            decision['score'] * 0.04 +
            reporting['score'] * 0.03
        )
        
        # Quality gate failure overrides everything
        if quality['grade'] == 'FAIL':
            overall_grade = 'FAIL'
        elif integration['grade'] == 'FAIL':
            overall_grade = 'FAIL'
        elif weighted_score >= 90:
            overall_grade = 'EXCELLENT'
        elif weighted_score >= 80:
            overall_grade = 'GOOD'
        elif weighted_score >= 70:
            overall_grade = 'PASS'
        else:
            overall_grade = 'FAIL'
        
        return {
            'weighted_score': weighted_score,
            'grade': overall_grade,
            'critical_failure': quality['grade'] == 'FAIL' or integration['grade'] == 'FAIL'
        }
    
    def are_states_related(self, predicted, actual):
        """Check if predicted and actual states are reasonably related"""
        
        related_states = {
            'WAVE_REVIEW': ['INTEGRATION', 'ERROR_RECOVERY'],
            'INTEGRATION': ['WAVE_REVIEW', 'SPAWN_AGENTS'],
            'WAVE_START': ['SPAWN_AGENTS', 'PLANNING'],
            'SPAWN_AGENTS': ['MONITOR', 'WAVE_START']
        }
        
        return actual in related_states.get(predicted, [])
    
    def evaluate_decision_rationale(self, rationale):
        """Evaluate quality of decision rationale"""
        
        if not rationale or len(rationale.strip()) < 10:
            return 0  # No or minimal rationale
        
        # Check for key reasoning elements
        reasoning_indicators = [
            'quality gates', 'architect review', 'integration',
            'size compliance', 'dependencies', 'phase complete'
        ]
        
        indicators_present = sum(
            1 for indicator in reasoning_indicators
            if indicator.lower() in rationale.lower()
        )
        
        return min(100, (indicators_present / len(reasoning_indicators)) * 150)
    
    def validate_report_accuracy(self, report, completion_data):
        """Validate report data matches actual completion data"""
        
        accuracy_checks = []
        
        # Check wave ID consistency
        reported_wave = report.get('wave_id')
        actual_wave = f"phase{completion_data.get('phase', 'X')}_wave{completion_data.get('wave', 'Y')}"
        accuracy_checks.append(reported_wave == actual_wave)
        
        # Check effort count consistency
        reported_efforts = len(report.get('effort_details', []))
        actual_efforts = len(completion_data.get('efforts', []))
        accuracy_checks.append(reported_efforts == actual_efforts)
        
        # Check quality gate consistency
        report_gates = report.get('quality_gates', {})
        actual_gates = completion_data.get('quality_gates', {})
        accuracy_checks.append(
            report_gates.get('all_passed') == actual_gates.get('all_passed')
        )
        
        return (sum(accuracy_checks) / len(accuracy_checks)) * 100
    
    def evaluate_report_usefulness(self, report):
        """Evaluate how useful the report is for future planning"""
        
        usefulness_elements = [
            'lessons_learned' in report.get('summary', {}),
            'recommendations' in report,
            'performance_metrics' in report and len(report['performance_metrics']) > 3,
            'next_wave_readiness' in report.get('summary', {}),
            report.get('quality_gates', {}).get('failed_gates') is not None
        ]
        
        return (sum(usefulness_elements) / len(usefulness_elements)) * 100
```

## Wave Completion Performance Tracking

```yaml
# Update orchestrator-state.yaml
grading:
  WAVE_COMPLETE:
    latest:
      timestamp: "2025-08-23T17:45:30Z"
      wave_id: "phase1_wave2"
      quality_gates_passed: 100
      validation_duration_minutes: 8.5
      integration_conflicts: 0
      decision_accuracy: 100
      report_completeness: 95
      overall: "EXCELLENT"
      
    history:
      - {timestamp: "...", wave: "phase1_wave1", grade: "GOOD", gates: 100, duration: 12.1}
      - {timestamp: "...", wave: "phase1_wave2", grade: "EXCELLENT", gates: 100, duration: 8.5}
      
    cumulative:
      waves_completed: 6
      excellent: 4
      good: 2
      pass: 0
      fail: 0
      avg_validation_minutes: 9.7
      quality_gate_success_rate: 100
      avg_integration_conflicts: 0.3
```

## Warning Triggers

┌─────────────────────────────────────────────────────────────────┐
│ WAVE COMPLETION WARNINGS                                       │
├─────────────────────────────────────────────────────────────────┤
│ Any Quality Gate Failure:                                     │
│ ❌ CRITICAL: Wave completion invalid                           │
│ ❌ Failed gates: {list}                                       │
│ ❌ Wave must be corrected before proceeding                   │
│                                                                 │
│ Validation Time >10 minutes:                                  │
│ ⚠️ WARNING: Slow wave validation                              │
│ ⚠️ Duration: {time} (target: <10 min)                        │
│ ⚠️ Review validation processes                                │
│                                                                 │
│ Integration Conflicts:                                         │
│ ⚠️⚠️ WARNING: Wave integration issues                        │
│ ⚠️⚠️ Conflicts may indicate coordination problems            │
│                                                                 │
│ Decision Accuracy <90%:                                       │
│ ⚠️ WARNING: State transition decision issues                  │
│ ⚠️ Review decision-making criteria                           │
└─────────────────────────────────────────────────────────────────┘

## Performance Optimization

```python
def optimize_wave_completion_performance():
    """Guidelines for excellent wave completion grades"""
    
    optimization_strategies = {
        'quality_gate_optimization': [
            'Implement continuous quality validation during wave',
            'Use automated checks for size compliance',
            'Set up real-time test coverage monitoring',
            'Create pre-completion validation checkpoints'
        ],
        
        'validation_speed_optimization': [
            'Parallelize validation checks where possible',
            'Cache validation results that don\'t change',
            'Use incremental validation approaches',
            'Pre-validate during development'
        ],
        
        'integration_optimization': [
            'Implement dependency-aware merge ordering',
            'Use automated conflict detection',
            'Pre-analyze potential conflicts',
            'Maintain integration readiness checks'
        ],
        
        'decision_optimization': [
            'Implement decision trees for state transitions',
            'Use historical data to improve decisions',
            'Document decision rationale thoroughly',
            'Validate decisions against actual outcomes'
        ]
    }
    
    return optimization_strategies
```

## Automated Quality Gate Validation

```python
class QualityGateValidator:
    def __init__(self):
        self.validation_rules = self.load_validation_rules()
        
    def validate_wave_completion(self, wave_data):
        """Run all quality gate validations"""
        
        validation_results = {
            'gates': {},
            'all_passed': True,
            'failed_gates': [],
            'warnings': []
        }
        
        # Size compliance validation
        size_result = self.validate_size_compliance(wave_data)
        validation_results['gates']['size_compliance'] = size_result
        if not size_result['passed']:
            validation_results['all_passed'] = False
            validation_results['failed_gates'].append('Size compliance failed')
        
        # Test coverage validation
        coverage_result = self.validate_test_coverage(wave_data)
        validation_results['gates']['test_coverage'] = coverage_result
        if not coverage_result['passed']:
            validation_results['all_passed'] = False
            validation_results['failed_gates'].append('Test coverage insufficient')
        
        # Integration validation
        integration_result = self.validate_integration_clean(wave_data)
        validation_results['gates']['integration'] = integration_result
        if not integration_result['passed']:
            validation_results['all_passed'] = False
            validation_results['failed_gates'].append('Integration issues detected')
        
        # Documentation validation
        docs_result = self.validate_documentation_complete(wave_data)
        validation_results['gates']['documentation'] = docs_result
        if not docs_result['passed']:
            validation_results['all_passed'] = False
            validation_results['failed_gates'].append('Documentation incomplete')
        
        return validation_results
    
    def validate_size_compliance(self, wave_data):
        """Validate all efforts meet size requirements"""
        
        size_issues = []
        
        for effort in wave_data.get('efforts', []):
            effort_branch = effort.get('branch')
            if effort_branch:
                try:
                    result = subprocess.run([
                        '/workspaces/kcp-shared-tools/tmc-pr-line-counter.sh',
                        '-c', effort_branch
                    ], capture_output=True, text=True, check=True)
                    
                    lines = int(result.stdout.strip().split()[-1])
                    if lines > 800:
                        size_issues.append({
                            'effort': effort['id'],
                            'lines': lines,
                            'limit': 800
                        })
                        
                except Exception as e:
                    size_issues.append({
                        'effort': effort['id'],
                        'error': f'Size check failed: {e}'
                    })
        
        return {
            'passed': len(size_issues) == 0,
            'issues': size_issues,
            'total_efforts_checked': len(wave_data.get('efforts', []))
        }
```

## Real-Time Grade Dashboard

```python
def generate_wave_completion_dashboard():
    """Generate real-time wave completion performance dashboard"""
    
    current_wave = get_current_wave_data()
    grader = WaveCompleteGrader()
    grade_data = grader.grade_wave_completion(current_wave)
    
    dashboard = {
        'current_grade': grade_data,
        'quality_gate_status': get_quality_gate_status(),
        'validation_progress': get_validation_progress(),
        'historical_performance': get_wave_completion_trends()
    }
    
    print("📊 WAVE COMPLETION PERFORMANCE DASHBOARD")
    print(f"Current Grade: {grade_data['overall']['grade']}")
    print(f"Quality Gates: {'✅' if grade_data['quality_gates']['pass_percentage'] == 100 else '❌'} {grade_data['quality_gates']['pass_percentage']:.1f}%")
    print(f"Validation Speed: {grade_data['validation_speed']['duration_minutes']:.1f} min")
    print(f"Integration: {'✅' if grade_data['integration']['grade'] != 'FAIL' else '❌'} {grade_data['integration']['final_status']}")
    
    return dashboard
```