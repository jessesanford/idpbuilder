# Orchestrator - INTEGRATION State Grading

## Critical Performance Metrics

┌─────────────────────────────────────────────────────────────────┐
│ PRIMARY METRIC: Integration Success Rate                       │
├─────────────────────────────────────────────────────────────────┤
│ Measurement: Clean merges without conflicts                    │
│ Target: >95% success rate                                      │
│ Grade: PASS/FAIL (binary)                                     │
│ Weight: 40% of overall orchestrator grade                     │
│ Consequence: FAIL = Architecture review required              │
└─────────────────────────────────────────────────────────────────┘

## Grading Rubric

| Metric | Excellent | Good | Acceptable | FAIL |
|--------|-----------|------|------------|------|
| Integration Success | 100% clean | 98-100% | 95-97% | <95% |
| Conflict Resolution | <5 min | 5-15 min | 15-30 min | >30 min |
| Test Pass Rate | 100% | 99% | 95% | <95% |
| Size Compliance | 100% | 100% | 100% | <100% |
| Integration Speed | <5 min | 5-10 min | 10-20 min | >20 min |

## Real-Time Scoring

```python
class IntegrationGrader:
    def __init__(self):
        self.integration_attempts = []
        self.conflict_resolutions = []
        self.size_validations = []
        
    def grade_integration_attempt(self, integration_data):
        """Grade a wave integration attempt"""
        
        # Critical: Success rate
        success_grade = self.calculate_success_rate(integration_data)
        
        # Speed metrics
        speed_grade = self.calculate_integration_speed(integration_data)
        
        # Conflict resolution
        resolution_grade = self.evaluate_conflict_resolution(integration_data)
        
        # Size compliance
        size_grade = self.validate_size_compliance(integration_data)
        
        # Test results
        test_grade = self.evaluate_test_results(integration_data)
        
        overall = self.calculate_overall_grade(
            success_grade, speed_grade, resolution_grade, size_grade, test_grade
        )
        
        return {
            'success': success_grade,
            'speed': speed_grade,
            'resolution': resolution_grade,
            'size': size_grade,
            'tests': test_grade,
            'overall': overall,
            'timestamp': datetime.now().isoformat()
        }
    
    def calculate_success_rate(self, data):
        """Calculate integration success rate"""
        total_efforts = len(data['efforts_included'])
        conflicts = data['conflicts_detected']
        
        success_rate = ((total_efforts - conflicts) / total_efforts) * 100
        
        if success_rate == 100:
            grade = 'EXCELLENT'
            score = 100
        elif success_rate >= 98:
            grade = 'GOOD'
            score = 90
        elif success_rate >= 95:
            grade = 'PASS'
            score = 75
        else:
            grade = 'FAIL'
            score = 0
        
        return {
            'success_rate': success_rate,
            'conflicts': conflicts,
            'grade': grade,
            'score': score
        }
    
    def calculate_integration_speed(self, data):
        """Calculate integration completion speed"""
        start_time = datetime.fromisoformat(data['started_at'].replace('Z', '+00:00'))
        end_time = datetime.fromisoformat(data['completed_at'].replace('Z', '+00:00'))
        
        duration_minutes = (end_time - start_time).total_seconds() / 60
        
        if duration_minutes < 5:
            grade = 'EXCELLENT'
            score = 100
        elif duration_minutes < 10:
            grade = 'GOOD'
            score = 90
        elif duration_minutes < 20:
            grade = 'PASS'
            score = 75
        else:
            grade = 'FAIL'
            score = 0
        
        return {
            'duration_minutes': duration_minutes,
            'grade': grade,
            'score': score
        }
    
    def evaluate_conflict_resolution(self, data):
        """Evaluate conflict resolution efficiency"""
        conflicts = data['conflicts_detected']
        
        if conflicts == 0:
            return {
                'conflicts': 0,
                'resolution_time': '0s',
                'grade': 'EXCELLENT',
                'score': 100
            }
        
        # Parse resolution time
        resolution_time = data['resolution_time']
        if 'min' in resolution_time:
            minutes = float(resolution_time.replace('min', ''))
        else:
            minutes = 0  # Assume seconds, convert if needed
        
        if minutes < 5:
            grade = 'EXCELLENT'
            score = 100
        elif minutes < 15:
            grade = 'GOOD'
            score = 85
        elif minutes < 30:
            grade = 'PASS'
            score = 70
        else:
            grade = 'FAIL'
            score = 0
        
        return {
            'conflicts': conflicts,
            'resolution_minutes': minutes,
            'grade': grade,
            'score': score
        }
    
    def validate_size_compliance(self, data):
        """Validate size limits maintained"""
        final_size = int(data['final_size_check'].split()[0])
        
        # Must be under limits (assume 800 line limit)
        if final_size <= 800:
            grade = 'PASS'
            score = 100
        else:
            grade = 'FAIL'
            score = 0
        
        return {
            'final_size': final_size,
            'limit': 800,
            'compliant': final_size <= 800,
            'grade': grade,
            'score': score
        }
    
    def evaluate_test_results(self, data):
        """Evaluate test suite results"""
        tests = data['test_results']
        
        # All must pass
        all_pass = all(result == 'PASS' for result in tests.values())
        
        if all_pass:
            grade = 'PASS'
            score = 100
        else:
            grade = 'FAIL'
            score = 0
        
        return {
            'results': tests,
            'all_pass': all_pass,
            'grade': grade,
            'score': score
        }
    
    def calculate_overall_grade(self, success, speed, resolution, size, tests):
        """Calculate weighted overall grade"""
        # Integration success: 40%
        # Size compliance: 30% (critical)
        # Test results: 20%
        # Speed: 10%
        
        weighted_score = (
            success['score'] * 0.40 +
            size['score'] * 0.30 +
            tests['score'] * 0.20 +
            speed['score'] * 0.10
        )
        
        # Critical failures override
        if success['grade'] == 'FAIL' or size['grade'] == 'FAIL' or tests['grade'] == 'FAIL':
            overall_grade = 'FAIL'
        elif weighted_score >= 90:
            overall_grade = 'EXCELLENT'
        elif weighted_score >= 80:
            overall_grade = 'GOOD'
        elif weighted_score >= 70:
            overall_grade = 'PASS'
        else:
            overall_grade = 'FAIL'
        
        return {
            'weighted_score': weighted_score,
            'grade': overall_grade
        }
```

## Integration Metrics Tracking

```yaml
# Update orchestrator-state.json
grading:
  INTEGRATION:
    latest:
      timestamp: "2025-08-23T15:05:22Z"
      success_rate: 100.0
      conflicts: 0
      duration_minutes: 5.4
      size_compliant: true
      tests_passing: true
      overall: "EXCELLENT"
    history:
      - {timestamp: "...", grade: "PASS", success: 95.2, conflicts: 1}
      - {timestamp: "...", grade: "EXCELLENT", success: 100.0, conflicts: 0}
    cumulative:
      attempts: 12
      excellent: 8
      good: 3
      pass: 1
      fail: 0
      avg_success_rate: 98.7
```

## Warning Conditions

┌─────────────────────────────────────────────────────────────────┐
│ INTEGRATION WARNINGS                                           │
├─────────────────────────────────────────────────────────────────┤
│ Success Rate <98%:                                            │
│ ⚠️ WARNING: Integration conflicts detected                     │
│ ⚠️ Review effort coordination and dependencies                 │
│                                                                │
│ Multiple Conflicts:                                           │
│ ⚠️⚠️ PATTERN WARNING: Recurring integration issues           │
│ ⚠️⚠️ Architecture review recommended                          │
│                                                                │
│ Size Compliance Failure:                                      │
│ ❌ CRITICAL: Integration exceeds size limits                  │
│ ❌ Immediate split or refactoring required                    │
└─────────────────────────────────────────────────────────────────┘

## Performance Optimization

```python
def optimize_integration_performance():
    """Guidelines for excellent integration grades"""
    
    best_practices = {
        'pre_integration': [
            'Validate all efforts pass tests before integration',
            'Check for potential conflicts using git analysis',
            'Ensure all efforts are size-compliant',
            'Coordinate shared dependency changes'
        ],
        
        'during_integration': [
            'Merge efforts in dependency order',
            'Run incremental tests after each merge',
            'Monitor integration branch size continuously',
            'Document any manual conflict resolutions'
        ],
        
        'post_integration': [
            'Full test suite validation',
            'Size compliance verification',
            'Performance impact assessment',
            'Documentation updates'
        ]
    }
    
    return best_practices
```

## Grade Reporting

Integration grades should be reported immediately after completion:

```python
def report_integration_grade(wave_id, grade_data):
    """Report integration grade to orchestrator state"""
    
    print(f"🔗 INTEGRATION COMPLETE - {wave_id}")
    print(f"Grade: {grade_data['overall']['grade']}")
    print(f"Success Rate: {grade_data['success']['success_rate']:.1f}%")
    print(f"Duration: {grade_data['speed']['duration_minutes']:.1f} minutes")
    print(f"Conflicts: {grade_data['resolution']['conflicts']}")
    print(f"Size Compliant: {grade_data['size']['compliant']}")
    print(f"Tests Passing: {grade_data['tests']['all_pass']}")
    
    if grade_data['overall']['grade'] == 'FAIL':
        print("❌ INTEGRATION FAILED - Recovery required")
    elif grade_data['overall']['grade'] == 'EXCELLENT':
        print("✅ EXCELLENT - Ready for next phase")
```