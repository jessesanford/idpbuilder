# Code Reviewer - CODE_REVIEW State Grading

## Critical Performance Metrics

┌─────────────────────────────────────────────────────────────────┐
│ PRIMARY METRIC: Review Accuracy and Thoroughness              │
├─────────────────────────────────────────────────────────────────┤
│ Measurement: Quality of review decisions and issue detection  │
│ Target: 100% critical issues caught, 95% accuracy            │
│ Grade: EXCELLENT/GOOD/PASS/FAIL                               │
│ Weight: 70% of overall code reviewer grade                    │
│ Consequence: Missed issues cause downstream failures          │
└─────────────────────────────────────────────────────────────────┘

## Grading Rubric

| Metric | Excellent | Good | Acceptable | FAIL |
|--------|-----------|------|------------|------|
| Critical Issue Detection | 100% caught | 95% caught | 90% caught | <90% |
| Size Compliance Accuracy | 100% accurate | 100% accurate | 100% accurate | <100% |
| KCP Pattern Validation | 95% complete | 90% complete | 85% complete | <85% |
| Test Coverage Assessment | 100% accurate | 95% accurate | 90% accurate | <90% |
| Review Decision Quality | 100% correct | 95% correct | 90% correct | <90% |

## Real-Time Scoring

```python
class CodeReviewGrader:
    def __init__(self):
        self.review_weights = {
            'critical_issue_detection': 0.35,
            'size_compliance_accuracy': 0.25,
            'kcp_pattern_validation': 0.20,
            'test_coverage_assessment': 0.15,
            'review_decision_quality': 0.05
        }
        
    def grade_code_review(self, review_data):
        """Grade a code review cycle"""
        
        # Critical issue detection accuracy
        issue_detection = self.assess_issue_detection_accuracy(review_data)
        
        # Size compliance validation accuracy
        size_accuracy = self.assess_size_compliance_accuracy(review_data)
        
        # KCP pattern validation completeness
        kcp_validation = self.assess_kcp_validation_quality(review_data)
        
        # Test coverage assessment accuracy
        coverage_assessment = self.assess_coverage_assessment_accuracy(review_data)
        
        # Overall review decision quality
        decision_quality = self.assess_review_decision_quality(review_data)
        
        overall = self.calculate_overall_review_grade(
            issue_detection, size_accuracy, kcp_validation,
            coverage_assessment, decision_quality
        )
        
        return {
            'issue_detection': issue_detection,
            'size_accuracy': size_accuracy,
            'kcp_validation': kcp_validation,
            'coverage_assessment': coverage_assessment,
            'decision_quality': decision_quality,
            'overall': overall,
            'timestamp': datetime.now().isoformat()
        }
    
    def assess_issue_detection_accuracy(self, review_data):
        """Assess accuracy of critical issue detection"""
        
        review_findings = review_data.get('review_findings', {})
        ground_truth = review_data.get('ground_truth_issues', {})  # From later validation
        
        if not ground_truth:
            # No ground truth available yet - assess based on review thoroughness
            return self.assess_review_thoroughness(review_findings)
        
        # Compare detected issues with ground truth
        critical_issues_found = len(review_findings.get('critical_issues', []))
        critical_issues_actual = len(ground_truth.get('critical_issues', []))
        
        high_issues_found = len(review_findings.get('high_issues', []))
        high_issues_actual = len(ground_truth.get('high_issues', []))
        
        # Calculate detection accuracy
        if critical_issues_actual > 0:
            critical_detection_rate = min(100, (critical_issues_found / critical_issues_actual) * 100)
        else:
            critical_detection_rate = 100 if critical_issues_found == 0 else 0
        
        if high_issues_actual > 0:
            high_detection_rate = min(100, (high_issues_found / high_issues_actual) * 100)
        else:
            high_detection_rate = 100 if high_issues_found == 0 else 0
        
        # Weight critical issues more heavily
        overall_detection = (critical_detection_rate * 0.8) + (high_detection_rate * 0.2)
        
        # Check for false positives
        false_positive_penalty = self.calculate_false_positive_penalty(
            review_findings, ground_truth
        )
        
        adjusted_score = max(0, overall_detection - false_positive_penalty)
        
        if adjusted_score >= 95:
            grade = 'EXCELLENT'
        elif adjusted_score >= 90:
            grade = 'GOOD'
        elif adjusted_score >= 80:
            grade = 'PASS'
        else:
            grade = 'FAIL'
        
        return {
            'critical_detection_rate': critical_detection_rate,
            'high_detection_rate': high_detection_rate,
            'overall_detection_score': overall_detection,
            'false_positive_penalty': false_positive_penalty,
            'adjusted_score': adjusted_score,
            'grade': grade,
            'issues_found': critical_issues_found + high_issues_found,
            'issues_actual': critical_issues_actual + high_issues_actual
        }
    
    def assess_size_compliance_accuracy(self, review_data):
        """Assess accuracy of size compliance validation"""
        
        size_review = review_data.get('size_compliance', {})
        
        # Critical requirements for size compliance
        requirements_met = {
            'used_correct_tool': size_review.get('tool_used') == 'tmc-pr-line-counter.sh',
            'measured_lines': 'actual_lines' in size_review,
            'documented_measurement': 'raw_output' in size_review or 'raw_measurement' in size_review,
            'correct_decision': self.validate_size_decision(size_review)
        }
        
        # Size compliance is binary - all requirements must be met
        all_requirements_met = all(requirements_met.values())
        
        if all_requirements_met:
            grade = 'EXCELLENT'
            score = 100
        else:
            grade = 'FAIL'
            score = 0
        
        missing_requirements = [req for req, met in requirements_met.items() if not met]
        
        return {
            'requirements_met': requirements_met,
            'all_requirements_met': all_requirements_met,
            'missing_requirements': missing_requirements,
            'tool_used': size_review.get('tool_used', 'UNKNOWN'),
            'lines_measured': size_review.get('actual_lines', 0),
            'grade': grade,
            'score': score
        }
    
    def assess_kcp_validation_quality(self, review_data):
        """Assess quality of KCP pattern validation"""
        
        kcp_review = review_data.get('kcp_compliance', {})
        
        validation_areas = {
            'multi_tenancy': assess_multi_tenancy_validation(kcp_review),
            'api_export_integration': assess_api_export_validation(kcp_review),
            'workspace_isolation': assess_workspace_isolation_validation(kcp_review),
            'syncer_compatibility': assess_syncer_validation(kcp_review),
            'rbac_patterns': assess_rbac_validation(kcp_review),
            'resource_quotas': assess_quota_validation(kcp_review)
        }
        
        # Calculate weighted score for KCP validation areas
        area_scores = [score for score in validation_areas.values() if score is not None]
        if area_scores:
            average_score = sum(area_scores) / len(area_scores)
        else:
            average_score = 0
        
        # Check for KCP-specific issues identified
        kcp_issues = kcp_review.get('kcp_issues', [])
        critical_kcp_issues = [issue for issue in kcp_issues if issue.get('severity') == 'CRITICAL']
        
        # Bonus for identifying KCP-specific patterns and issues
        pattern_bonus = 0
        if len(kcp_issues) > 0:
            pattern_bonus = min(10, len(kcp_issues) * 2)  # Bonus for finding KCP issues
        
        final_score = min(100, average_score + pattern_bonus)
        
        if final_score >= 95:
            grade = 'EXCELLENT'
        elif final_score >= 90:
            grade = 'GOOD'
        elif final_score >= 85:
            grade = 'PASS'
        else:
            grade = 'FAIL'
        
        return {
            'validation_areas': validation_areas,
            'average_area_score': average_score,
            'kcp_issues_found': len(kcp_issues),
            'critical_kcp_issues': len(critical_kcp_issues),
            'pattern_bonus': pattern_bonus,
            'final_score': final_score,
            'grade': grade
        }
    
    def assess_coverage_assessment_accuracy(self, review_data):
        """Assess accuracy of test coverage assessment"""
        
        coverage_review = review_data.get('test_coverage', {})
        actual_coverage = review_data.get('actual_coverage_data', {})  # From automated tools
        
        if not actual_coverage:
            # Assess based on thoroughness of coverage review
            return self.assess_coverage_review_thoroughness(coverage_review)
        
        # Compare reviewer assessment with actual data
        reviewer_score = coverage_review.get('coverage_score', 0)
        actual_score = actual_coverage.get('overall_coverage', 0)
        
        # Calculate accuracy of coverage assessment
        if actual_score > 0:
            accuracy = max(0, 100 - abs(reviewer_score - actual_score))
        else:
            accuracy = 0
        
        # Check specific coverage area assessments
        area_accuracy = {}
        coverage_areas = ['unit_tests', 'integration_tests', 'multi_tenant_tests']
        
        for area in coverage_areas:
            reviewer_assessment = coverage_review.get(area, {})
            actual_data = actual_coverage.get(area, {})
            
            if actual_data:
                area_accuracy[area] = self.compare_coverage_assessments(
                    reviewer_assessment, actual_data
                )
            else:
                area_accuracy[area] = 75  # Default for unavailable data
        
        # Weight overall accuracy
        overall_accuracy = (accuracy * 0.5) + (sum(area_accuracy.values()) / len(area_accuracy) * 0.5)
        
        if overall_accuracy >= 95:
            grade = 'EXCELLENT'
        elif overall_accuracy >= 90:
            grade = 'GOOD'
        elif overall_accuracy >= 85:
            grade = 'PASS'
        else:
            grade = 'FAIL'
        
        return {
            'reviewer_score': reviewer_score,
            'actual_score': actual_score,
            'accuracy': accuracy,
            'area_accuracy': area_accuracy,
            'overall_accuracy': overall_accuracy,
            'grade': grade
        }
    
    def assess_review_decision_quality(self, review_data):
        """Assess quality of final review decision"""
        
        review_decision = review_data.get('review_decision', {})
        subsequent_issues = review_data.get('subsequent_issues', [])  # Issues found after approval
        
        decision_result = review_decision.get('result', 'UNKNOWN')
        blocking_issues = review_decision.get('blocking_issues', [])
        
        # Check decision appropriateness
        decision_quality = {
            'decision_appropriate': self.validate_decision_appropriateness(review_decision),
            'blocking_issues_valid': self.validate_blocking_issues(blocking_issues),
            'recommendations_quality': self.assess_recommendations_quality(review_decision.get('recommendations', [])),
            'subsequent_issues_missed': len(subsequent_issues)
        }
        
        # Calculate decision quality score
        base_score = 0
        if decision_quality['decision_appropriate']:
            base_score += 40
        if decision_quality['blocking_issues_valid']:
            base_score += 30
        if decision_quality['recommendations_quality'] > 75:
            base_score += 20
        
        # Penalty for missing issues that cause problems later
        issue_penalty = min(50, decision_quality['subsequent_issues_missed'] * 10)
        final_score = max(0, base_score + 10 - issue_penalty)  # +10 for base participation
        
        if final_score >= 90:
            grade = 'EXCELLENT'
        elif final_score >= 80:
            grade = 'GOOD'
        elif final_score >= 70:
            grade = 'PASS'
        else:
            grade = 'FAIL'
        
        return {
            'decision_components': decision_quality,
            'base_score': base_score,
            'issue_penalty': issue_penalty,
            'final_score': final_score,
            'grade': grade,
            'decision_result': decision_result
        }
    
    def calculate_overall_review_grade(self, detection, size, kcp, coverage, decision):
        """Calculate weighted overall review grade"""
        
        # Use weights defined in __init__
        weighted_score = (
            detection['adjusted_score'] * self.review_weights['critical_issue_detection'] +
            size['score'] * self.review_weights['size_compliance_accuracy'] +
            kcp['final_score'] * self.review_weights['kcp_pattern_validation'] +
            coverage['overall_accuracy'] * self.review_weights['test_coverage_assessment'] +
            decision['final_score'] * self.review_weights['review_decision_quality']
        )
        
        # Critical failure conditions override everything
        critical_failures = []
        if size['grade'] == 'FAIL':
            critical_failures.append('Size compliance validation failed')
        if detection['adjusted_score'] < 90:
            critical_failures.append('Critical issue detection insufficient')
        if kcp['grade'] == 'FAIL':
            critical_failures.append('KCP pattern validation inadequate')
        
        # Determine final grade
        if critical_failures:
            overall_grade = 'FAIL'
        elif weighted_score >= 95:
            overall_grade = 'EXCELLENT'
        elif weighted_score >= 85:
            overall_grade = 'GOOD'
        elif weighted_score >= 75:
            overall_grade = 'PASS'
        else:
            overall_grade = 'FAIL'
        
        return {
            'weighted_score': weighted_score,
            'grade': overall_grade,
            'critical_failures': critical_failures,
            'review_effectiveness': overall_grade in ['EXCELLENT', 'GOOD', 'PASS']
        }
    
    def validate_size_decision(self, size_review):
        """Validate that size compliance decision was correct"""
        
        actual_lines = size_review.get('actual_lines', 0)
        compliant_decision = size_review.get('compliant', None)
        
        # Correct decision logic
        should_be_compliant = actual_lines <= 800
        
        return compliant_decision == should_be_compliant
    
    def assess_review_thoroughness(self, review_findings):
        """Assess thoroughness when ground truth isn't available"""
        
        thoroughness_indicators = {
            'size_checked': 'size_compliance' in review_findings,
            'tests_reviewed': 'test_coverage' in review_findings,
            'kcp_patterns_checked': 'kcp_compliance' in review_findings,
            'security_reviewed': 'security_review' in review_findings,
            'architecture_validated': 'architecture_review' in review_findings,
            'performance_assessed': 'performance_review' in review_findings
        }
        
        thoroughness_score = sum(thoroughness_indicators.values()) / len(thoroughness_indicators) * 100
        
        # Bonus for detailed findings
        total_issues = sum(len(findings) for findings in review_findings.values() if isinstance(findings, list))
        detail_bonus = min(10, total_issues)  # Max 10 points for detailed findings
        
        final_score = min(100, thoroughness_score + detail_bonus)
        
        if final_score >= 95:
            grade = 'EXCELLENT'
        elif final_score >= 85:
            grade = 'GOOD'
        elif final_score >= 75:
            grade = 'PASS'
        else:
            grade = 'FAIL'
        
        return {
            'thoroughness_indicators': thoroughness_indicators,
            'thoroughness_score': thoroughness_score,
            'detail_bonus': detail_bonus,
            'final_score': final_score,
            'grade': grade
        }
```

## Performance Tracking

```yaml
# Update orchestrator-state.json
grading:
  CODE_REVIEWER_CODE_REVIEW:
    latest:
      timestamp: "2025-08-23T19:45:00Z"
      effort_id: "phase1-wave2-effort3-webhooks"
      issue_detection_accuracy: 94
      size_compliance_accuracy: 100
      kcp_validation_quality: 91
      coverage_assessment_accuracy: 88
      decision_quality: 92
      overall: "GOOD"
      
    history:
      - {timestamp: "...", effort: "effort1", grade: "EXCELLENT", detection: 98}
      - {timestamp: "...", effort: "effort2", grade: "GOOD", detection: 92}
      
    cumulative:
      reviews_completed: 12
      excellent: 5
      good: 6
      pass: 1
      fail: 0
      avg_detection_rate: 93.8
      avg_size_accuracy: 100
      critical_issues_missed: 0
```

## Warning Triggers

┌─────────────────────────────────────────────────────────────────┐
│ CODE REVIEW PERFORMANCE WARNINGS                              │
├─────────────────────────────────────────────────────────────────┤
│ Critical Issue Detection <90%:                                 │
│ ❌ CRITICAL: Missing critical implementation issues            │
│ ❌ Detection rate: {rate}% (target: >95%)                    │
│ ❌ Review effectiveness compromised                           │
│                                                                 │
│ Size Compliance Validation Error:                              │
│ 🚨 CRITICAL: Size compliance not properly validated           │
│ 🚨 Must use tmc-pr-line-counter.sh tool                      │
│ 🚨 Size violations will cause downstream failures             │
│                                                                 │
│ KCP Pattern Validation <85%:                                  │
│ ⚠️⚠️ WARNING: KCP compliance validation insufficient         │
│ ⚠️⚠️ Multi-tenancy issues may escape detection              │
│ ⚠️⚠️ Architectural violations possible                       │
│                                                                 │
│ Review Decision Accuracy <90%:                                 │
│ ⚠️ WARNING: Review decisions inconsistent                     │
│ ⚠️ May approve code with issues or block good code           │
│ ⚠️ Review calibration needed                                 │
└─────────────────────────────────────────────────────────────────┘

## Performance Optimization

```python
def optimize_code_review_performance():
    """Guidelines for excellent code review grades"""
    
    optimization_strategies = {
        'issue_detection_optimization': [
            'Use comprehensive checklists for each review area',
            'Study common KCP/Kubernetes anti-patterns',
            'Maintain database of recurring issue types',
            'Cross-reference with previous review findings'
        ],
        
        'size_compliance_optimization': [
            'ALWAYS use tmc-pr-line-counter.sh tool',
            'Document measurement process completely',
            'Verify tool output before making decisions',
            'Never estimate size - always measure'
        ],
        
        'kcp_validation_optimization': [
            'Maintain KCP pattern compliance checklist',
            'Study KCP architecture documentation regularly',
            'Validate multi-tenancy in every implementation',
            'Check APIExport integration thoroughly'
        ],
        
        'decision_quality_optimization': [
            'Base decisions on objective criteria',
            'Document rationale for all decisions',
            'Consider downstream impact of approvals',
            'Balance thoroughness with development velocity'
        ]
    }
    
    return optimization_strategies
```

## Automated Review Quality Validation

```python
class ReviewQualityValidator:
    def __init__(self):
        self.validation_rules = self.load_validation_rules()
        
    def validate_review_quality(self, review_data):
        """Run automated validation of review quality"""
        
        validation_results = {
            'completeness': self.validate_review_completeness(review_data),
            'accuracy': self.validate_review_accuracy(review_data),
            'consistency': self.validate_review_consistency(review_data),
            'thoroughness': self.validate_review_thoroughness(review_data)
        }
        
        overall_quality = self.calculate_review_quality_score(validation_results)
        
        return {
            'validation_results': validation_results,
            'overall_quality': overall_quality,
            'quality_issues': self.identify_quality_issues(validation_results),
            'improvement_recommendations': self.generate_improvement_recommendations(validation_results)
        }
    
    def validate_review_completeness(self, review_data):
        """Validate that review covered all required areas"""
        
        required_areas = [
            'size_compliance', 'test_coverage', 'kcp_compliance',
            'security_review', 'architecture_review', 'review_decision'
        ]
        
        completeness_results = {}
        for area in required_areas:
            completeness_results[area] = {
                'present': area in review_data,
                'detailed': len(str(review_data.get(area, {}))) > 100,
                'actionable': self.has_actionable_findings(review_data.get(area, {}))
            }
        
        return completeness_results
    
    def validate_review_accuracy(self, review_data):
        """Validate accuracy of review findings against known benchmarks"""
        
        accuracy_checks = {}
        
        # Size compliance accuracy (can be verified immediately)
        size_data = review_data.get('size_compliance', {})
        if 'actual_lines' in size_data:
            accuracy_checks['size_measurement'] = {
                'tool_correct': size_data.get('tool_used') == 'tmc-pr-line-counter.sh',
                'decision_logical': self.validate_size_decision_logic(size_data),
                'documented_properly': 'raw_output' in size_data or 'raw_measurement' in size_data
            }
        
        return accuracy_checks
```

## Real-Time Grade Dashboard

```python
def generate_code_review_dashboard():
    """Generate real-time code review performance dashboard"""
    
    current_review = get_current_review_data()
    grader = CodeReviewGrader()
    grade_data = grader.grade_code_review(current_review)
    
    dashboard = {
        'current_grade': grade_data,
        'issue_detection_trend': get_detection_accuracy_trend(),
        'size_compliance_streak': get_size_compliance_streak(),
        'kcp_validation_effectiveness': get_kcp_validation_trends()
    }
    
    print("📊 CODE REVIEW PERFORMANCE DASHBOARD")
    print(f"Current Grade: {grade_data['overall']['grade']}")
    print(f"Issue Detection: {grade_data['issue_detection']['adjusted_score']:.1f}%")
    print(f"Size Compliance: {'✅' if grade_data['size_accuracy']['grade'] != 'FAIL' else '❌'} {grade_data['size_accuracy']['score']:.1f}%")
    print(f"KCP Validation: {grade_data['kcp_validation']['final_score']:.1f}%")
    
    return dashboard
```